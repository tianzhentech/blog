<!DOCTYPE html><html lang="zh"><head><meta name="google-site-verification" content="6dKXSVBE3ZyVxHtiFCvc57UwP5MEzcp51X7Wn5okSoE"><link rel="icon" href="/api/images/tianzhen.jpg"><script>document.addEventListener("DOMContentLoaded",function(){var e;768<=(window.innerWidth||document.documentElement.clientWidth||document.body.clientWidth)&&((e=document.createElement("script")).src="/api/js/fireworks.min.js",document.head.appendChild(e),(e=document.createElement("script")).src="/api/js/background.min.js",document.head.appendChild(e),(e=document.createElement("script")).src="/api/js/cursor.min.js",document.head.appendChild(e),(e=document.createElement("canvas")).id="fireworks",e.style.cssText="position: fixed; top: 0; left: 0; width: 100vw; height: 100vh; pointer-events: none; z-index: 32767",document.body.appendChild(e),(e=document.createElement("canvas")).id="background",e.style.cssText="position: fixed; top: 0; left: 0; width: 100vw; height: 100vh; pointer-events: none; z-index: -1",document.body.appendChild(e),(e=document.createElement("div")).id="cursor",document.body.appendChild(e))})</script><meta charset="utf-8"><title>基于PyTorch的手写数字识别（持续补充中......） | Tianzhen</title><meta name="author" content="Tianzhen"><meta name="description" content="My Learning Journey Blog"><meta name="keywords" content="seo优化 javaweb教程 网络技术分享"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0"><link rel="preconnect" href="https://cdn.staticfile.org"><link rel="preconnect" href="https://twikoo.lt514.com"><link rel="preconnect" href="https://umami.hoshiroko.com"><link rel="preconnect" href="/api"><script src="/api/js/vue.global.prod.min.js"></script><script src="/api/js/anime.min.js"></script><script async src="https://umami.hoshiroko.com/script.js" data-website-id="6c968d09-0bb6-4304-a28c-7944d8474aad"></script><link rel="stylesheet" href="/api/fontawesome/fontawesome6.5.1/css/all.min.css"><link rel="stylesheet" href="/api/css/fonts/fonts.loli.net.min.css"><script>const mixins={}</script><script src="/api/js/highlight.min.js"></script><script src="/api/js/highlightjs-line-numbers.min.js"></script><link rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/11.9.0/styles/github.min.css"><script src="/api/js/lib/highlight.js"></script><script src="https://cdn.staticfile.org/KaTeX/0.16.9/katex.min.js"></script><script src="https://cdn.staticfile.org/KaTeX/0.16.9/contrib/auto-render.min.js"></script><link rel="stylesheet" href="https://cdn.staticfile.org/KaTeX/0.16.9/katex.min.css"><script src="/api/js/lib/math.js"></script><script src="/api/js/lib/preview.js"></script><script src="/api/js/twikoo.all.min.js"></script><link rel="stylesheet" href="/api/css/main.css"><style>.flex-container{display:flex;justify-content:center;align-items:center;height:100vh}</style><meta name="generator" content="Hexo 6.3.0"></head><script>var st,OriginTitile=document.title;document.addEventListener("visibilitychange",function(){document.hidden?(document.title="o(இ௰இ)怎么就走了！",clearTimeout(st)):(document.title="☆*o(≧▽≦)o*☆欢迎回来！",st=setTimeout(function(){document.title=OriginTitile},3e3))})</script><body><div id="layout"><transition name="fade"><div id="loading" v-show="loading"><div id="loading-circle"><h2>LOADING</h2><p>加载过慢请开启缓存 浏览器默认开启</p><img src="/api/images/loading.gif"></div></div></transition><div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}"><nav id="desktop-menu"><a class="title" href="/"><span>TIANZHEN</span> </a><a href="/"><i class="fa-solid fa-house fa-fw"></i> <span>&ensp;Home</span> </a><a href="/about"><i class="fa-solid fa-id-card fa-fw"></i> <span>&ensp;About</span> </a><a href="/archives"><i class="fa-solid fa-box-archive fa-fw"></i> <span>&ensp;Archives</span> </a><a href="/categories"><i class="fa-solid fa-bookmark fa-fw"></i> <span>&ensp;Categories</span> </a><a href="/tags"><i class="fa-solid fa-tags fa-fw"></i> <span>&ensp;Tags</span> </a><a target="_blank" rel="noopener" href="https://umami.hoshiroko.com/share/jpoapxn7zGXoKcSv/tianzhentech"><i class="fa-solid fa-chart-column fa-fw"></i> <span>&ensp;Umami</span></a></nav><nav id="mobile-menu"><div class="title" @click="showMenuItems = !showMenuItems"><i class="fa-solid fa-bars fa-fw"></i> <span>&emsp;TIANZHEN</span></div><transition name="slide"><div class="items" v-show="showMenuItems"><a href="/"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-house fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Home</div></div></a><a href="/about"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-id-card fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">About</div></div></a><a href="/archives"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-box-archive fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Archives</div></div></a><a href="/categories"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-bookmark fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Categories</div></div></a><a href="/tags"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-tags fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Tags</div></div></a></div></transition></nav></div><transition name="fade"><div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div></transition><div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'"><div class="article"><div><h1>基于PyTorch的手写数字识别（持续补充中......）</h1></div><div class="info"><span class="date"><span class="icon"><i class="fa-solid fa-calendar fa-fw"></i> </span>2024/5/9 </span><span class="category"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="icon"><i class="fa-solid fa-bookmark fa-fw"></i> </span>机器学习 </a></span><span class="tags"><span class="icon"><i class="fa-solid fa-tags fa-fw"></i> </span><span class="tag"><a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="color:#ff7d73">神经网络</a> </span><span class="tag"><a href="/tags/CNN/" style="color:#03a9f4">CNN</a> </span><span class="tag"><a href="/tags/CUDA/" style="color:#00bcd4">CUDA</a> </span><span class="tag"><a href="/tags/PyTorch/" style="color:#03a9f4">PyTorch</a></span></span></div><div class="content" v-pre><h1><span id="1-概览">1. 概览</span></h1><h2><span id="11-总体介绍">1.1. 总体介绍</span></h2><p>周志华教授曾表达过这样的思想：快速入门一个学科，不是把一个学科，一本书里面的一个章节研究的非常透彻，而是快速通读全书，大致了解整本书在讲些什么，然后在后续的学习中持续发掘其中的细节，直至慢慢掌握其精髓。以我的理解，就是快速写出它的“hello world”，大概就能一定程度上体会到它在讲些什么。</p><p>而手写数字识别，作为神经网络的入门课，就扮演着“hello world”这样的角色。</p><h2><span id="12-名词解释">1.2. 名词解释</span></h2><blockquote><p>为后续理解作基本的铺垫</p></blockquote><ul><li><p><strong>CNN</strong>，全称卷积神经网络（Convolutional Neural Network），是一种深度学习的算法，特别适合于处理图像数据。CNN在图像识别、语音识别等许多领域都有广泛的应用。</p><p>CNN的主要特点是它可以自动地学习到从原始数据中提取哪些特征是重要的，这是通过训练过程中的权重调整实现的。这与传统的机器学习方法不同，传统的机器学习方法通常需要人工选择和提取特征。</p><p>CNN的基本组成部分包括卷积层、池化层（Pooling Layer）和全连接层（Fully Connected Layer）。卷积层用于从输入数据中提取局部特征，池化层用于降低数据的维度，全连接层则用于将学习到的特征进行最终的分类或回归。</p></li><li><p><strong>MNIST</strong>（Modified National Institute of Standards and Technology database）是一个广泛用于训练各种图像处理系统的大型手写数字数据库。它包含60,000个训练样本和10,000个测试样本，每个样本都是一个28x28像素的灰度图像，代表了0到9的一个数字。</p><p>MNIST数据集是由美国国家标准与技术研究院（NIST）的原始数据集修改和混合而来的。它的目标是作为一个简单的计算机视觉基准数据集，用于科研人员测试和比较不同的机器学习和图像处理算法。</p><p>MNIST数据集的一个重要特点是，它的样本已经进行了预处理，包括中心化、归一化等，这使得它非常适合用于基础的机器学习和深度学习实验。因此，MNIST数据集经常被用作教学和研究的基准数据集，特别是在图像识别和机器学习的初级教学中。</p></li><li><p><strong>Ground Truth</strong> 是一个术语，通常在机器学习和计算机视觉领域中使用，指的是数据的真实标签或真实值。在监督学习中，我们通常有一组带有标签的训练数据，这些标签就是 “ground truth”。这些真实的标签用于训练模型，使模型能够学习到输入数据和输出标签之间的映射关系。</p><p>例如，在图像分类任务中，每张图片的 “ground truth” 就是图片的真实类别；在物体检测任务中，每个物体的 “ground truth” 是物体的类别和其在图像中的位置（通常表示为一个边界框）；在回归任务中，每个输入的 “ground truth” 是一个连续的数值。</p><p>“Ground truth” 是评估模型性能的基础。通过将模型的预测结果与 “ground truth” 进行比较，我们可以计算出各种性能指标，如准确率、召回率、F1分数、均方误差等，以了解模型的性能如何。</p></li><li><p><strong>threshold</strong>，在神经网络和其他机器学习模型中，”threshold” 是一个常见的术语，用来描述一个决定性的界限。当某个值超过这个界限时，模型的行为或输出会发生改变。</p></li><li><p><strong>CUDA</strong>（Compute Unified Device Architecture）是由NVIDIA公司开发的一种并行计算平台和应用程序接口（API）。它允许开发者使用NVIDIA的图形处理单元（GPU）进行通用计算。</p><p>在过去，GPU主要用于渲染计算机图形，但由于其强大的并行处理能力，现在已经被广泛用于其他计算密集型任务，如科学计算、机器学习和深度学习等。</p><p>CUDA提供了一种相对简单的方式来编写并行代码，使得开发者可以更容易地利用GPU的计算能力。在深度学习中，使用CUDA可以显著加速训练和推理过程，特别是对于需要大量计算的任务，如训练复杂的神经网络模型。</p><p>值得注意的是，虽然CUDA是NVIDIA开发的，但许多深度学习框架（如TensorFlow和PyTorch）都支持使用CUDA，这使得它们可以在NVIDIA的GPU上运行。</p></li><li><p><strong>cuDNN</strong>，全称CUDA Deep Neural Network library，是由NVIDIA开发的一个深度神经网络GPU加速库。它是专门为深度神经网络的前向和反向传播过程提供优化的库，包括卷积、池化、归一化、激活层等常见的神经网络操作。</p><p>cuDNN的主要优点是可以在NVIDIA的CUDA-enabled GPUs上提供高效的计算性能。它提供了低级的API，可以直接进行底层的操作，也提供了高级的API，可以方便地进行常见的操作。</p><p>许多深度学习框架，如TensorFlow、PyTorch、Caffe等，都使用cuDNN来加速在GPU上的计算。使用cuDNN可以大大提高深度学习模型的训练和推理速度。</p></li><li><p><strong>PyTorch</strong>是一个开源的机器学习库，由Facebook的人工智能研究团队开发。它提供了两个主要功能：</p><ol><li>一个n维数组库，类似于NumPy，但可以在GPU上运行以进行高效的数值计算。</li><li>自动微分系统（用于实现神经网络），支持动态计算图，这使得它在构建和训练复杂的深度学习模型时具有很大的灵活性。</li></ol><p>PyTorch的一个主要特点是其动态计算图系统。在许多其他深度学习库中（如TensorFlow和Theano），计算图在运行前需要被完全定义并编译。相比之下，PyTorch允许你在运行时动态地改变计算图，这使得它更易于理解和调试，也更适合处理变长的输入数据。</p><p>PyTorch还提供了一系列的工具和库，包括用于数据加载和预处理的torchvision库，用于机器学习的torchtext库，以及用于科学计算和高级优化的torch.optim库等。</p><p>PyTorch的设计理念是简洁、灵活和直观，这使得它在研究社区中非常受欢迎，同时也逐渐在工业界得到应用。</p></li><li><p><strong>Kaggle</strong>是一个在线社区，为数据科学家和机器学习工程师提供一个平台，他们可以在这个平台上发现和分享数据集，探索和构建模型，进行数据科学和机器学习竞赛，以及发布和分享他们的工作。</p><p>Kaggle的主要特点包括：</p><ol><li><p><strong>竞赛</strong>：Kaggle最初是作为一个平台开始的，公司和研究者可以在上面发布数据科学竞赛。这些竞赛涵盖了各种问题，从图像分类到文本分析，从预测模型到推荐系统。参与者可以提交他们的解决方案，并在公共排行榜上与其他参与者竞争。许多竞赛还提供现金奖励。</p></li><li><p><strong>数据集</strong>：Kaggle提供了一个平台，用户可以发布、搜索和下载数据集。这些数据集涵盖了各种主题，包括（但不限于）公共卫生、经济学、图像识别、自然语言处理等。</p></li><li><p><strong>Kernels&#x2F;Notebooks</strong>：Kaggle提供了一个在线代码执行环境，称为Kaggle Kernels（现在更名为Kaggle Notebooks）。用户可以在这个环境中编写Python或R代码，进行数据分析和建模，然后分享他们的代码和结果。</p></li><li><p><strong>讨论论坛</strong>：Kaggle社区有一个活跃的讨论论坛，用户可以在这里讨论竞赛、数据集、编程问题，以及数据科学和机器学习的最新趋势。</p></li></ol><p>Kaggle是一个很好的资源，无论你是一个数据科学新手，还是一个有经验的专业人士，都可以在这里学习、分享和发展你的技能。</p></li><li><p><strong>超参数</strong>，在机器学习中，超参数是在开始学习过程之前设置的参数，而不是通过训练得到的参数。换句话说，超参数是模型训练之前由数据科学家或机器学习工程师手动设置的参数。</p><p>超参数可以影响模型的学习过程和模型的性能。例如，对于神经网络，超参数可能包括学习率（决定模型学习的速度），批次大小（每次训练步骤中用于更新模型权重的样本数量），以及训练的轮数（整个数据集通过模型的次数）等。</p><p>另一个例子是支持向量机（SVM）的惩罚参数C和核函数的参数。这些超参数控制模型的复杂性和灵活性。</p><p>选择合适的超参数是机器学习中的一个重要任务，因为不同的超参数可能会导致模型的性能有很大的差异。这个过程通常需要大量的实验和调整，也可以使用一些自动化的方法，如网格搜索、随机搜索或贝叶斯优化等。</p></li><li><p><strong>batch</strong>，在机器学习和深度学习中，”batch”是指用于一次模型更新的数据集的子集。批处理大小（batch size）是一个超参数，定义了每次迭代（或更新）中用于计算梯度的样本数量。</p><p>例如，假设你有一个包含1000个样本的训练集，如果你设置批处理大小为100，那么你的模型将在每个训练周期（epoch）中进行10次更新，每次更新使用100个样本。</p><p>批处理的主要优点是：</p><ol><li><p><strong>计算效率</strong>：使用批处理可以利用并行处理能力，比如GPU，从而提高计算效率。</p></li><li><p><strong>稳定的梯度估计</strong>：使用更大的批处理可以得到更稳定（但可能不是更准确）的梯度估计。</p></li><li><p><strong>内存使用</strong>：对于大型数据集，可能无法一次性将所有数据加载到内存中，批处理可以有效地管理内存使用。</p></li></ol><p>然而，选择批处理大小也是一个权衡。较小的批处理可能会导致更频繁的更新，可能会导致训练过程更快地收敛，但也可能导致梯度估计的噪声更大。另一方面，较大的批处理可能会导致更稳定的梯度估计，但可能需要更多的内存，并且可能需要更多的训练周期才能收敛。</p></li><li><p><strong>epoch</strong>，在机器学习和深度学习中，”epoch”是指整个训练集通过模型一次的过程。换句话说，一个epoch就是模型看过训练集中的每一个样本一次。</p><p>例如，如果你有一个包含1000个样本的训练集，那么一个epoch就是模型对这1000个样本进行一次前向传播和一次反向传播。</p><p>在训练深度学习模型时，通常需要进行多个epoch，因为一次通过所有的训练数据可能不足以让模型学习到数据中的所有模式。通过多次迭代训练数据，模型可以更好地学习和适应数据。</p><p>需要注意的是，epoch的数量也是一个超参数，需要根据具体的任务和数据来设定。太少的epoch可能会导致模型欠拟合，而太多的epoch可能会导致模型过拟合。</p></li><li><p><strong>损失函数</strong>（也被称为代价函数或误差函数）是一个用于衡量机器学习模型预测结果与真实值之间差距的函数。换句话说，它度量了模型的预测错误程度。在训练过程中，我们的目标是最小化这个损失函数。</p><p>损失函数的选择取决于你正在解决的具体问题。例如，对于回归问题（预测一个连续的输出），常见的损失函数是均方误差（Mean Squared Error，MSE），它计算的是模型预测值和真实值之间差值的平方的平均值。对于分类问题（预测一个离散的输出），常见的损失函数是交叉熵损失（Cross-Entropy Loss），它度量的是模型预测的概率分布与真实的概率分布之间的差距。</p><p>损失函数是机器学习中的一个核心概念，因为它定义了我们的优化目标。通过优化算法（如梯度下降），我们可以调整模型的参数以最小化损失函数，从而提高模型的预测性能。</p></li><li><p><strong>反向传播</strong>（Backpropagation）是一种在神经网络中用于训练模型的关键算法。它的主要目标是通过有效地计算梯度来优化损失函数，这个梯度是损失函数相对于模型权重的偏导数。</p><p>反向传播的过程可以分为两个主要步骤：</p><ol><li><p><strong>前向传播</strong>：在这个阶段，输入数据通过网络向前传播，通过每一层的神经元和连接权重，直到生成输出。然后，这个输出与期望的输出（标签）进行比较，计算出损失函数的值。</p></li><li><p><strong>反向传播</strong>：在这个阶段，算法从输出层开始，向后通过网络，计算损失函数相对于每个权重的偏导数（即梯度）。这个过程是通过链式法则（Chain Rule）来完成的，链式法则是微积分中的一个基本原则。</p></li></ol><p>这些梯度然后被用于更新网络的权重，通常是通过一种叫做梯度下降的优化算法。这个过程在整个训练数据集上重复多次（也就是多个”epoch”），直到模型的性能达到满意的水平或者不再显著提高。</p><p>反向传播是深度学习中的一个核心算法，使得训练深度神经网络成为可能。</p></li><li><p><strong>激活函数</strong>在神经网络中起着非常重要的作用。它们被应用于神经元的输出上，以引入非线性因素，使得神经网络能够学习并执行更复杂的任务。</p><p>如果没有激活函数，无论神经网络有多少层，它都只能表示线性变换，这大大限制了网络的表达能力。通过引入非线性激活函数，神经网络可以学习并表示更复杂的模式。</p><p>以下是一些常见的激活函数：</p><ol><li><strong>Sigmoid函数</strong>：Sigmoid函数可以将任何值转换为0到1之间的值，使其可以用于输出层，以表示概率。但是，Sigmoid函数在输入值的绝对值很大时，梯度接近于0，这可能导致梯度消失问题。Sigmoid函数是一类函数而不是一个函数，意为“S型”，我们最常用的Sigmoid函数是$f(x) &#x3D; \frac{1}{1 + e^{-x}}$。</li></ol><p><img loading="lazy" src="/../images/image-20240509211312083.png" alt="image-20240509211312083"></p><ol start="2"><li><strong>ReLU（Rectified Linear Unit）函数</strong>：ReLU函数在输入值为负时输出0，在输入值为正时输出输入值本身。ReLU函数简单且计算效率高，但是在输入值为负时，梯度为0，可能导致神经元”死亡”。</li></ol><p><img loading="lazy" src="/../images/image-20240509211350318.png" alt="image-20240509211350318"></p><ol start="3"><li><strong>Tanh函数</strong>：Tanh函数可以将任何值转换为-1到1之间的值，比Sigmoid函数的输出范围更广。但是，Tanh函数也存在梯度消失的问题。</li></ol><p><img loading="lazy" src="/../images/image-20240509211432202.png" alt="image-20240509211432202"></p><ol start="4"><li><strong>Leaky ReLU函数</strong>：为了解决ReLU函数的”死亡”神经元问题，Leaky ReLU在输入值为负时，会有一个小的正斜率。</li></ol><p><img loading="lazy" src="/../images/image-20240509211609072.png" alt="image-20240509211609072"></p><ol start="5"><li><strong>Softmax函数</strong>：Softmax函数可以将一组值转换为概率分布，常用于多分类问题的输出层。</li></ol><p>选择哪种激活函数取决于具体的应用和网络结构。</p></li></ul><h1><span id="2-安装cuda与cudnn">2. 安装CUDA与cuDNN</span></h1><blockquote><p>虽然cpu也可以训练，但是有gpu不用白不用，效率还更高</p></blockquote><h2><span id="21-安装cuda">2.1. 安装CUDA</span></h2><p>首先确保有一个Nvidia独显，在cmd中输入</p><pre><code class="cmd">nvidia-smi
</code></pre><p><img loading="lazy" src="/../images/image-20240509215822771.png" alt="image-20240509215822771"></p><p>这里显示的版本是当前显卡支持的最高版本，但是我看<a target="_blank" rel="noopener" href="https://pytorch.org/">PyTorch</a>好像还没有适配12.4版本的CUDA，PyTorch官网支持到12.1版本，所以我们安装12.1的：</p><p><img loading="lazy" src="/../images/image-20240509220129278.png" alt="image-20240509220129278"></p><p>打开<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-toolkit-archive">CUDA Toolkit Archive | NVIDIA Developer</a>，我们下载12.1.1版本：</p><p><img loading="lazy" src="/../images/image-20240509220942991.png" alt="image-20240509220942991"></p><p><img loading="lazy" src="/../images/image-20240509221125473.png" alt="image-20240509221125473"></p><p>根据自己的需求下载。</p><p>然后点击自定义，一般全部安装就行：</p><p><img loading="lazy" src="/../images/image-20240509221342208.png" alt="image-20240509221342208"></p><p><img loading="lazy" src="/../images/image-20240509221355652.png" alt="image-20240509221355652"></p><p>然后打开Nvidia Experience，更新驱动到最新版</p><p><img loading="lazy" src="/../images/image-20240509221509950.png" alt="image-20240509221509950"></p><p>因为我之前下载过，没有这个软件的可以去<a target="_blank" rel="noopener" href="https://www.nvidia.cn/geforce/geforce-experience/download/">下载</a>。</p><p>安装完成之后在cmd中输入</p><pre><code class="cmd">nvcc -V
</code></pre><p><img loading="lazy" src="/../images/image-20240509223038314.png" alt="image-20240509223038314"></p><p>看到版本信息说明安装成功，如果没有输出这个可能是环境变量的问题：</p><p>打开系统属性：</p><p><img loading="lazy" src="/../images/image-20240509223423735.png" alt="image-20240509223423735"></p><p>看看系统变量里是否有这个变量：</p><p><img loading="lazy" src="/../images/image-20240509223518076.png" alt="image-20240509223518076"></p><p>没有就手动添加一下，正常情况下安装就会自动设置环境变量。</p><h2><span id="22-安装cudnn">2.2. 安装cuDNN</span></h2><p>安装cuDNN可以加速，新版本的cuDNN已经不需要手动配置环境变量，直接<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cudnn-downloads?target_os=Windows&target_arch=x86_64&target_version=10">下载exe版本</a>一步一步安装即可。</p><p><img loading="lazy" src="/../images/image-20240509224002175.png" alt="image-20240509224002175"></p><p><img loading="lazy" src="/../images/image-20240509224640481.png" alt="image-20240509224640481"></p><p><img loading="lazy" src="/../images/image-20240509224659280.png" alt="image-20240509224659280"></p><p>我个人喜欢全部安装，自己按需选择。</p><p>验证是否安装成功，在这个目录下执行cmd：</p><p><img loading="lazy" src="/../images/image-20240509225300857.png" alt="image-20240509225300857"></p><p>输入bandwidthTest.exe和deviceQuery.exe，分别看到PASS就说明安装成功：</p><p><img loading="lazy" src="/../images/image-20240509225520925.png" alt="image-20240509225520925"></p><p><img loading="lazy" src="/../images/image-20240509225549038.png" alt="image-20240509225549038"></p><h1><span id="3-安装pytorch-cuda版本">3. 安装PyTorch CUDA版本</span></h1><blockquote><p>在PyCharm中，导入torch默认是导入cpu的版本</p></blockquote><p>可以运行这段代码</p><pre><code class="python">import torch
print(torch.__version__)
</code></pre><p>如果版本后缀是cpu就说明安装的是cpu版本，我们需要CUDA版本的PyTorch，依据<a target="_blank" rel="noopener" href="https://pytorch.org/">PyTorch</a>官网的安装方式，推荐使用pip或者在Anaconda中安装，注意自己的CUDA版本：</p><p><img loading="lazy" src="/../images/image-20240509230938890.png" alt="image-20240509230938890"></p><p>理论上pip是最快的，但是在下载中经常出现中断，可以使用中国大陆的镜像或者使用代理安装，如果都不能安装成功也可以手动下载whl安装。</p><p>在<a target="_blank" rel="noopener" href="https://download.pytorch.org/whl/torch_stable.html">这个</a>页面，下载相应的版本：</p><p><img loading="lazy" src="/../images/image-20240509233754857.png" alt="image-20240509233754857"></p><p>前面的cu121表示CUDA12.1版本，后面的cp310表示3.10的python版本，后面表示win，amd64，选择适合自己的版本。</p><p>然后拖动到项目目录下用<code>pip install</code>安装。</p><blockquote><p>在PyCharm中验证</p></blockquote><pre><code class="python">import torch

if torch.cuda.is_available():
    print(&quot;PyTorch can use GPUs!&quot;)
else:
    print(&quot;PyTorch cannot use GPUs.&quot;)

print(torch.__version__)
</code></pre><p><img loading="lazy" src="/../images/image-20240509234136188.png" alt="image-20240509234136188"></p><p>然后就大功告成了。</p><h1><span id="4-原理">4. 原理</span></h1><h2><span id="41-神经网络概述">4.1. 神经网络概述</span></h2><blockquote><p>本节内容引用周志华老师的课件</p></blockquote><p><img loading="lazy" src="/../images/image-20240509235532510.png"></p><h3><span id="411-m-p神经元模型">4.1.1. M-P神经元模型</span></h3><blockquote><p>“MP神经元”是指McCulloch-Pitts神经元，这是一种最早的、最简单的人工神经元模型。它是由Warren McCulloch和Walter Pitts在1943年提出的，因此得名。</p></blockquote><p>还记得生物学中的神经元是怎么工作的吗：</p><ol><li><strong>接收信号</strong>：神经元通过其树突（一个或多个分支状的结构）接收来自其他神经元的信号。这些信号是化学物质，称为神经递质，它们从其他神经元的突触（神经元之间的连接点）释放出来。</li><li><strong>整合信号</strong>：神经元的细胞体（含有细胞核的部分）将接收到的所有信号进行整合。如果这些信号的总和超过了一个特定的阈值，那么神经元就会被激活。</li><li><strong>传递信号</strong>：一旦神经元被激活，就会在其轴突（从细胞体延伸出的长线状结构）上产生一个电信号，称为动作电位。动作电位会沿着轴突向下传播，直到达到轴突的末端。</li><li><strong>信号传输</strong>：在轴突的末端，动作电位会导致神经递质的释放。这些神经递质会穿过突触间隙，然后与接收神经元的突触后膜上的受体结合，从而传递信号给下一个神经元。</li></ol><blockquote><p>机器学习中的神经元借鉴了生物学中的概念</p></blockquote><p>对于每一个神经元，$x_i$为输入值，$w_i$是对应的权重，$\theta$是阈值（threshold），如果所有$wix_i$之和大于$\theta$，该神经元就被激活，即$f$函数（激活函数）生效。所以最终的输出为：</p><p>$$y&#x3D;f(\sum\limits_{i&#x3D;1}^{n} w_ix_i-\theta)$$</p><p><img loading="lazy" src="/../images/image-20240509235609648.png"></p><h3><span id="412-激活函数">4.1.2. 激活函数</span></h3><blockquote><p>Activation function，也叫响应函数或者挤压函数（Squeeze function）</p></blockquote><p>所谓响应也就是给一个输入值得到输出值，挤压函数指的是给一个$(-\infty, \infty)$的值，挤压到$(0,1)$之间，如下图（b）所示。</p><ul><li>理想激活函数是阶跃函数，$0$表示抑制神经元而$1$表示激活神经元。</li><li>阶跃函数具有不连续、不光滑等不好的性质，常用的是Sigmoid函数。</li></ul><p><img loading="lazy" src="/../images/image-20240510141705166.png" alt="image-20240510141705166"></p><p>特别的，对于最常见的Sigmoid函数$f(x) &#x3D; \frac{1}{1 + e^{-x}}$，有这样的特性：</p><p>$$f’(x)&#x3D;f(x)·(1-f(x))$$</p><p>那么$f(x)$可以理解为正的概率，$(1-f(x))$可以理解为负的概率，这对我们后面理解BP有帮助。</p><h3><span id="413-多层前馈网络结构">4.1.3. 多层前馈网络结构</span></h3><blockquote><p>如果这个多层前馈网络结构里面每一个神经元都是一个M-P神经元，我们就得到一个多层前馈神经网络</p></blockquote><p><img loading="lazy" src="/../images/image-20240510144156637.png" alt="image-20240510144156637"></p><p>对于隐层和输出层，才有激活函数的参与，所以也被叫做功能单元。</p><p><strong>万有逼近性（universal approximator）</strong></p><p>多层前馈网络有强大的表示能力。</p><blockquote><p>仅需一个包含足够多神经元的隐层，多层前馈神经网络就能以任意精度逼近任意复杂度的连续函数</p></blockquote><p>但是，如何设置隐层神经元数是未决问题（Open Problem），实际常用”试错法“。</p><p><em>注意，万有逼近性并非神经网络的独有特性！</em></p><h3><span id="414-bpbackpropagation误差逆传播算法">4.1.4. BP（BackPropagation：误差逆传播算法）</span></h3><p><img loading="lazy" src="/../images/image-20240510151921398.png" alt="image-20240510151921398"></p><p>具体而言，在神经网络中，激活函数被应用在每一层的输出上。具体来说，每个神经元的输出是其输入与权重的线性组合，然后通过激活函数进行非线性转换。这个过程在每一层的每个神经元上都会发生。</p><p>例如，假设我们有一个三层的神经网络（输入层、隐藏层和输出层）。在隐藏层，每个神经元的输入是输入层的输出与权重的线性组合，然后这个结果通过激活函数进行非线性转换，得到隐藏层的输出。在输出层，每个神经元的输入是隐藏层的输出与权重的线性组合，然后这个结果再次通过激活函数进行非线性转换，得到最终的输出。</p><p><strong>BP算法推导</strong></p><p>我们把上级的输出与权重的线性组合，叫做这级的输入$\beta$，也就是输出函数$y&#x3D;f(\sum\limits_{i&#x3D;1}^{n} w_ix_i-\theta)$里面的$\sum\limits_{i&#x3D;1}^{n} w_ix_i$，这么叫只是为了后面使公式简洁。</p><p>于是我们得到每个神经元上的预测值公式：</p><p>$$\hat{y}_j^k&#x3D;f(\beta_j-\theta_j)$$</p><p>其中$k$表示第$k$个样例，$j$表示第$j$个神经元，$\theta$表示阈值（threshold），$\hat{y}$即预测值。</p><p>则整个网络在$(x_k,y_k)$上的均方误差为</p><p>$$E_k&#x3D;\frac{1}{2}\sum\limits_{j&#x3D;1}^{l}(\hat{y}_j^k-y_j^k)^2$$</p><p>取$\frac12$是为了求导之后消去平方，$y$是真实值（ground truth）。</p><blockquote><p>所谓的训练数据就是确定一个模型中的所有参数</p></blockquote><p>对于下图中的网络，输入层有$d$个神经元，隐层有$q$个，输出层有$l$个，则从输出层到隐层有$d·q$个参数，从隐层到输出层有$q·l$个参数，对于功能神经元（隐层和输出层），还有$q+l$个阈值$\theta$（threshold），加起来就是$(d+l+1)q+l$个参数，我们在训练过程中就是要确定这么多个参数。</p><p><img loading="lazy" src="/../images/image-20240510183928932.png" alt="image-20240510183928932"></p><p>BP是一个迭代学习算法，在迭代的每一轮中采用<code>广义感知机学习规则</code>（多层前馈也常叫做多层感知机）：</p><p>$$v\leftarrow{v}+\Delta{v}$$</p><p>其中$\Delta{v}$就是调整量，之所以要调整，就是因为有误差。</p><p>怎么调整呢？</p><p><img loading="lazy" src="/../images/image-20240510192345940.png" alt="image-20240510192345940"></p><p>$w_{hj}$是我们要学习的一个参数(某个权重)，求偏导是为了确定梯度的方向，$\eta$决定走的步长。</p><p><img loading="lazy" src="/../images/image-20240510194407742.png" alt="image-20240510194407742"></p><p><img loading="lazy" src="/../images/image-20240510195230774.png" alt="image-20240510195230774"></p><p>其中$b_h$就是输出层的输入值，$g_j$可以看作输出层的神经元处理。</p><p><img loading="lazy" src="/../images/image-20240510200810549.png" alt="image-20240510200810549"></p><p>在了解各个参数是怎么调节之后，我们就知道神经网络是怎么训练的：</p><p>在输入$x$之后，整个模型会随机初始化一组参数，然后算出$\hat{y}$，然后根据$y$马上就可以算出$E$，然后根据$E$就可以得到$\Delta{w}$、$\Delta{\theta}$、$\Delta{v}$、$\Delta{\gamma}$等调整量，然后让这些值进行调整，然后不断进行这样一轮轮训练，直到$\hat{y}$收敛于$y$附近，我们的参数也随之确定下来。</p><p><strong>震荡现象</strong></p><p>在预测值不断逼近真实值的过程中，会发生震荡现象，比如时而大于真实值，时而小于真实值，在真实值附近震荡。</p><p>但是我们总归有办法让训练过程停止，避免一直震荡下去。比如设置训练的轮数，或者保留一组测试集，在每一轮训练完成之后拿模型在测试集和训练集上的性能做对比，如果差别很大就继续，否则小到一定程度就让它停止。</p><p>另外为了避免震荡现象比较严重，$\eta$不能太大，太小也不行，越小就意味着需要更多的训练轮数才能足够收敛。</p><p>而且$\eta$本身也可以在训练过程中灵活的调整，以加速收敛，比如训练的初期可以设置较大的$\eta$，然后慢慢减小$\eta$，以确保在收敛附近有较小的$\eta$。</p><h2><span id="42-卷积原理">4.2. 卷积原理</span></h2><blockquote><p>我们以手写数字识别这个模型为例来讲解CNN神经网络</p></blockquote><h1><span id="5-编写模型">5. 编写模型</span></h1><h1><span id="6-优化">6. 优化</span></h1><h1><span id="7-总结">7. 总结</span></h1></div><div id="comment"><div id="twikoo-container"></div></div></div><footer id="footer"><div id="footer-wrap"><div>&copy; 2023 - 2024 Tianzhen <span id="footer-icon"><i class="fa-solid fa-font-awesome fa-fw"></i> </span>&commat;Tianzhen</div><div>Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp; <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a></div><div><a href="https://icp.gov.moe/?keyword=20236514" target="_blank">萌ICP备20236514号</a> <a target="_blank" rel="noopener" href="https://beian.miit.gov.cn">豫ICP备2023040339号</a></div></div><div><span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("11/07/2023 00:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="本站已运行 "+dnum+" 天 ",document.getElementById("times").innerHTML=hnum+" 小时 "+mnum+" 分 "+snum+" 秒"}setInterval("createtime()",250)</script></div></footer></div><transition name="fade"><div id="preview" ref="preview" v-show="previewShow"><img id="preview-content" ref="previewContent"></div></transition></div><script src="/api/js/main.js"></script><script>twikoo.init({el:"#twikoo-container",envId:"https://twikoo.lt514.com",region:"",path:location.pathname,lang:"zh"})</script><p style="text-align:center;margin:0" id="saintwei"></p><script>var saintwei=function(r){var i="気：",n=["现在开始天晴了","天空比大海还要深，是个未知的世界","天气因你逆转，世界因你天晴","能遇见你真是太好了","不管晴天还是雨天，我只是想和你相遇","我只是想再一次的见到她啊","是你让我找到了存在的意义","无论你在哪里，我一定拼尽全力去见你","拜托了，我们就这样一直在一起","只是天空的样子，就能改变人们的心情","天空上是另一个世界，自古如此","我从来不知道，渴望蓝天的人居然有那么多","天气真的是很不可思议，光只是天空的模样就让人感动不已","这是一个只有我和她知道，关于这世界的秘密","100%的晴天女孩","在充满暴风雨的世界，一起勇敢地爱下去","那年夏天，在那个天空之上的我们，把这个世界的样貌，彻底的改变了","天空比大海还要深，是个未知的世界","重要的人，想见的人，无论晴雨，不管多远，都一定要去见你","晴天里有阳光，阳光总是充满温馨，相信有这么多朋友的厚爱和鼓励，晴天会永远阳光灿烂","天晴得像一张蓝纸，几片薄薄的白云，像被阳光晒化了似的，随风缓缓浮游着","天空澄碧，纤云不染，远山含黛，和风送暖","浅蓝色的天幕，像一幅洁净的丝绒，镶着黄色的金边","一场飘洒的雨后，阳光带着清醒的空气飞来，试问是哪位仙子的生日，阳光如此美丽","晴天的午后，夏日的阳光如水般音符一样灿烂的流动，湿澈了不同的妩媚的忧伤","天那么蓝，连一丝浮絮都没有，像被过滤了一切杂色，瑰丽地熠熠发光","神明啊!求你!求你!求你!让我再见她一次","比起晴空，我更需要阳菜，天气什么的，就这样疯狂下去也可以"].map(function(t){return t}),l=["rgb(110,64,170)","rgb(150,61,179)","rgb(191,60,175)","rgb(228,65,157)","rgb(254,75,131)","rgb(255,94,99)","rgb(255,120,71)","rgb(251,150,51)","rgb(226,183,47)","rgb(198,214,60)","rgb(175,240,91)","rgb(127,246,88)","rgb(82,246,103)","rgb(48,239,130)","rgb(29,223,163)","rgb(26,199,194)","rgb(35,171,216)","rgb(54,140,225)","rgb(76,110,219)","rgb(96,84,200)"],a={text:"",prefixP:-5,skillI:0,skillP:0,direction:"forward",delay:2,step:1};!function t(){var e=n[a.skillI];a.step?a.step--:(a.step=1,a.prefixP<i.length?(0<=a.prefixP&&(a.text+=i[a.prefixP]),a.prefixP++):"forward"===a.direction?a.skillP<e.length?(a.text+=e[a.skillP],a.skillP++):a.delay?a.delay--:(a.direction="backward",a.delay=2):0<a.skillP?(a.text=a.text.slice(0,-1),a.skillP--):(a.skillI=(a.skillI+1)%n.length,a.direction="forward")),r.textContent=a.text,r.appendChild(function(t){for(var e=document.createDocumentFragment(),r=0;r<t;r++){var i=document.createElement("span");i.textContent=String.fromCharCode(94*Math.random()+33),i.style.color=l[Math.floor(Math.random()*l.length)],e.appendChild(i)}return e}(a.prefixP<i.length?Math.min(5,5+a.prefixP):Math.min(5,e.length-a.skillP))),setTimeout(t,75)}()};saintwei(document.getElementById("saintwei"))</script></body></html>