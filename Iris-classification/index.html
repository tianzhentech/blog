<!DOCTYPE html><html lang="zh"><head><meta name="google-site-verification" content="6dKXSVBE3ZyVxHtiFCvc57UwP5MEzcp51X7Wn5okSoE"><link rel="icon" href="/api/images/tianzhen.jpg"><script>document.addEventListener("DOMContentLoaded",function(){var e;768<=(window.innerWidth||document.documentElement.clientWidth||document.body.clientWidth)&&((e=document.createElement("script")).src="/api/js/fireworks.min.js",document.head.appendChild(e),(e=document.createElement("script")).src="/api/js/background.min.js",document.head.appendChild(e),(e=document.createElement("script")).src="/api/js/cursor.min.js",document.head.appendChild(e),(e=document.createElement("canvas")).id="fireworks",e.style.cssText="position: fixed; top: 0; left: 0; width: 100vw; height: 100vh; pointer-events: none; z-index: 32767",document.body.appendChild(e),(e=document.createElement("canvas")).id="background",e.style.cssText="position: fixed; top: 0; left: 0; width: 100vw; height: 100vh; pointer-events: none; z-index: -1",document.body.appendChild(e),(e=document.createElement("div")).id="cursor",document.body.appendChild(e))})</script><meta charset="utf-8"><title>基于Adaboost的鸢尾花分类模型 | Tianzhen</title><meta name="author" content="Tianzhen"><meta name="description" content="My Learning Journey Blog"><meta name="keywords" content="seo优化 javaweb教程 网络技术分享"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0"><link rel="preconnect" href="https://cdn.staticfile.org"><link rel="preconnect" href="https://twikoo.lt514.com"><link rel="preconnect" href="https://umami.hoshiroko.com"><link rel="preconnect" href="/api"><script src="/api/js/vue.global.prod.min.js"></script><script src="/api/js/anime.min.js"></script><script async src="https://umami.hoshiroko.com/script.js" data-website-id="6c968d09-0bb6-4304-a28c-7944d8474aad"></script><link rel="stylesheet" href="/api/fontawesome/fontawesome6.5.1/css/all.min.css"><link rel="stylesheet" href="/api/css/fonts/fonts.loli.net.min.css"><script>const mixins={}</script><script src="/api/js/highlight.min.js"></script><script src="/api/js/highlightjs-line-numbers.min.js"></script><link rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/11.9.0/styles/github.min.css"><script src="/api/js/lib/highlight.js"></script><script src="https://cdn.staticfile.org/KaTeX/0.16.9/katex.min.js"></script><script src="https://cdn.staticfile.org/KaTeX/0.16.9/contrib/auto-render.min.js"></script><link rel="stylesheet" href="https://cdn.staticfile.org/KaTeX/0.16.9/katex.min.css"><script src="/api/js/lib/math.js"></script><script src="/api/js/lib/preview.js"></script><script src="/api/js/twikoo.all.min.js"></script><link rel="stylesheet" href="/api/css/main.css"><style>.flex-container{display:flex;justify-content:center;align-items:center;height:100vh}</style><meta name="generator" content="Hexo 6.3.0"></head><script>var st,OriginTitile=document.title;document.addEventListener("visibilitychange",function(){document.hidden?(document.title="o(இ௰இ)怎么就走了！",clearTimeout(st)):(document.title="☆*o(≧▽≦)o*☆欢迎回来！",st=setTimeout(function(){document.title=OriginTitile},3e3))})</script><body><div id="layout"><transition name="fade"><div id="loading" v-show="loading"><div id="loading-circle"><h2>LOADING</h2><p>加载过慢请开启缓存 浏览器默认开启</p><img src="/api/images/loading.gif"></div></div></transition><div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}"><nav id="desktop-menu"><a class="title" href="/"><span>TIANZHEN</span> </a><a href="/"><i class="fa-solid fa-house fa-fw"></i> <span>&ensp;Home</span> </a><a href="/about"><i class="fa-solid fa-id-card fa-fw"></i> <span>&ensp;About</span> </a><a href="/archives"><i class="fa-solid fa-box-archive fa-fw"></i> <span>&ensp;Archives</span> </a><a href="/categories"><i class="fa-solid fa-bookmark fa-fw"></i> <span>&ensp;Categories</span> </a><a href="/tags"><i class="fa-solid fa-tags fa-fw"></i> <span>&ensp;Tags</span> </a><a target="_blank" rel="noopener" href="https://umami.hoshiroko.com/share/jpoapxn7zGXoKcSv/tianzhentech"><i class="fa-solid fa-chart-column fa-fw"></i> <span>&ensp;Umami</span></a></nav><nav id="mobile-menu"><div class="title" @click="showMenuItems = !showMenuItems"><i class="fa-solid fa-bars fa-fw"></i> <span>&emsp;TIANZHEN</span></div><transition name="slide"><div class="items" v-show="showMenuItems"><a href="/"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-house fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Home</div></div></a><a href="/about"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-id-card fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">About</div></div></a><a href="/archives"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-box-archive fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Archives</div></div></a><a href="/categories"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-bookmark fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Categories</div></div></a><a href="/tags"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-tags fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Tags</div></div></a></div></transition></nav></div><transition name="fade"><div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div></transition><div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'"><div class="article"><div><h1>基于Adaboost的鸢尾花分类模型</h1></div><div class="info"><span class="date"><span class="icon"><i class="fa-solid fa-calendar fa-fw"></i> </span>2024/5/18 </span><span class="category"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="icon"><i class="fa-solid fa-bookmark fa-fw"></i> </span>机器学习 </a></span><span class="tags"><span class="icon"><i class="fa-solid fa-tags fa-fw"></i> </span><span class="tag"><a href="/tags/Adaboost/" style="color:#ffa2c4">Adaboost</a> </span><span class="tag"><a href="/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" style="color:#ff7d73">集成学习</a> </span><span class="tag"><a href="/tags/Boosting/" style="color:#00a596">Boosting</a></span></span></div><div class="content" v-pre><h1><span id="1-集成学习">1. 集成学习</span></h1><h2><span id="11-介绍">1.1. 介绍</span></h2><p>集成学习（Ensemble Learning）是一种机器学习策略，它结合了多个学习模型的预测以提高总体的预测准确性。这种方法基于这样一个观察：在许多情况下，一组模型的集成预测通常比任何单个模型的预测更准确。</p><p>集成学习的主要目标是结合多个模型以减少过拟合（通过增加模型的多样性），减少偏差（通过结合多个模型的预测），并提高预测的稳定性。</p><blockquote><p>我们把集成学习中的每个模型叫做一个弱学习器或者个体或者基学习器、基模型，有很多种叫法。</p></blockquote><h2><span id="12-同质和异质">1.2. 同质和异质</span></h2><p>在集成学习中，”同质”和”异质”通常用来描述集成中使用的基学习器（base learners）或基模型（base models）的类型。</p><ul><li><p><strong>同质集成（Homogeneous ensemble）</strong>：在同质集成中，所有的基学习器都是同一类型的。例如，随机森林就是一个同质集成，因为它是由多个决策树组成的。</p></li><li><p><strong>异质集成（Heterogeneous ensemble）</strong>：在异质集成中，基学习器可以是不同类型的。例如，一个异质集成可能包含决策树、支持向量机和神经网络等不同类型的模型。Stacking就是一个常见的异质集成方法，其中不同类型的模型被用作基学习器，然后一个元学习器（可以是另一种类型的模型）被用来结合这些基学习器的预测。</p></li></ul><p>同质和异质集成各有优点和缺点。同质集成通常更容易实现和优化，因为所有的基学习器都是同一类型的。然而，异质集成可能能够更好地利用不同类型的模型的优点，从而可能在某些任务上表现得更好。</p><h2><span id="13-误差-分歧分解error-ambiguity-decomposition">1.3. 误差-分歧分解（error-ambiguity decomposition）</span></h2><blockquote><p>个体要好而不同</p></blockquote><p>得到一个好的集成并不容易，要满足学习器之间多样性要高，而且每个学习器的误差要低。</p><p>可以用以下公式表示：</p><p><img loading="lazy" src="/../images/image-20240518224829701.png" alt="image-20240518224829701"></p><p>其中E是我们追求的值，E越小集成的性能越好，$\overline{E}$表示个体误差的平均值，$\overline{A}$表示个体差异的平均值。</p><p>理论上，计算出$\overline{E}$和$\overline{A}$之后，我们就可以通过不断减小模型的$\overline{E}$，增大模型的$\overline{A}$来得到一个最小的$E$值，但是实际上我们做不到，因为这个公式里面的<code>ambiguity</code>是从数学上强制定义的，要知道<code>ambiguity</code>必须知道理想的<code>Ground Truth</code>结果，如果不知道理想的<code>Ground Truth</code>，我们是无法得出<code>ambiguity</code>的，即上图所示，<code>ambiguity</code>并不是一个<code>operable</code>（可操作的）定义。</p><p>此外，只有在回归问题的<code>squared loss</code>下才可以得到上述分解公式，像分类问题的<code>0-1 loss</code>，是得不到上面的式子的。</p><p>误差-分歧分解公式只是在直觉上给了我们一个支撑，并不是一个可操作的公式，所以我们要研究很多有效的算法。</p><blockquote><p>实际上，要实现上面这些非常困难，因为增大差异性就意为着个体性能的损失，这是一个取舍问题。</p></blockquote><h2><span id="14-成功的集成学习方法">1.4. 成功的集成学习方法</span></h2><p><img loading="lazy" src="/../images/image-20240518235743429.png" alt="image-20240518235743429"></p><p>序列化方法是一种串行的方法，即一个一个执行，并行化方法是一种并行的方法，所有一起执行，对于序列化方法而言，其代表是AdaBoost，这是开端，而今天我们用的最多的其实是GradientBoost，其中XGBoost就是GradientBoost的一个实现。对于并行化方法，其开端是Bagging，但是今天最常用的是RF。</p><blockquote><p>AdaBoost 是 AdaptiveBoost 的缩写，表明该算法是具有适应性的提升算法。</p></blockquote><p>以AdaBoost为代表的Boosting家族，其原理大概如下：</p><p><img loading="lazy" src="/../images/image-20240519111203923.png" alt="image-20240519111203923"></p><p>对于一个数据集，先交由第一个基学习器学习，然后该训练集检验学习好的模型，分为预测正确和预测错误的样本，对于预测正确的样本，我们减小其权重，预测错误的样本，增大其权重，然后对该数据集进行重采样或者直接带权重变成一个新的数据集，使得新数据集中上一个学习器预测错误的样本比正确的样本更重要。然后由新的基学习器重点学习之前做错的样本，这样循环往复，使得后一个学习器更关注之前做错的样本。最后预测的结果是所有基学习器按照误差率的一个加权结果，AdaBoost采取<strong>加权多数表决的方法，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用</strong>。</p><p>Bagging家族的原理就更好理解了，对于原始的数据集，我们进行一个Bootstrap（重采样），使得交给每个基学习器的数据集并不相同但是都来自于原始数据集，然后总预测结果我们对每一个基学习器的预测进行同等的对待，这与之前Boosting有所不同。RF是Bagging的改进版，至今仍非常广泛的使用。</p><p><img loading="lazy" src="/../images/image-20240519112729370.png" alt="image-20240519112729370"></p><h2><span id="15-圣杯问题">1.5. 圣杯问题</span></h2><p><img loading="lazy" src="/../images/image-20240519114445064.png" alt="image-20240519114445064"></p><p>以上图为例，我们可以用$\frac{b+c}{a+b+c+d}$来衡量两个分类器的差异性，这是一种度量方式，也可以用$\frac{(b+c)-(a+d)}{a+b+c+d}$来作为另外一种度量，有很多种度量方式。</p><p>研究者提出了很多Diversity measure：</p><p><img loading="lazy" src="/../images/image-20240519115802651.png" alt="image-20240519115802651"></p><p>但是这些度量至今没有完全解决问题，或多或少都有自己的问题，所有刻画Diversity的方法都不能得到大家的完全认可，所以这件事还需要进一步的研究。</p><p><img loading="lazy" src="/../images/image-20240519120145063.png" alt="image-20240519120145063"></p><p>我们离真正解决Diversity measure还差的很远，这是集成学习的一个<code>holy grail problem</code>（圣杯问题）。一旦我们完美解决了Diversity measure，也即解决了$E&#x3D;\overline{E}-\overline{A}$中的$\overline{A}$，那么集成学习将没有秘密可言，要知道我们现在的机器学习和集成学习的关系，做一个好的模型几乎都离不开集成学习。</p><h2><span id="16-意义">1.6. 意义</span></h2><p>集成学习在深度学习的大环境下依然有意义，很多时候我们还是选择用深度学习做一个特征学习，把深度神经网络的倒数第二层输出给一个集成学习模型，比如一个随机森林（RF）或者XGBoost，这样比单纯用深度学习的效果要好。</p><h1><span id="2-基于adaboost的鸢尾花分类模型">2. 基于Adaboost的鸢尾花分类模型</span></h1><p>本例子基于<a target="_blank" rel="noopener" href="https://www.educoder.net/tasks/pem5k7u2/1567447/5i4ucafflvo6?coursesId=pem5k7u2">机器学习 — Adaboost (educoder.net)</a></p><h2><span id="21-数学原理">2.1. 数学原理</span></h2><p>上面已经简单的介绍了Adaboost的基本原理，这里从公式和数学角度介绍一下Adaboost具体该怎么实现：</p><ol><li><p><strong>初始化样本权重</strong>：给每个训练样本$(x_1,x_2,…,x_N)$分配权重，初始权重$w_1$均为$1&#x2F;N$。</p></li><li><p><strong>训练弱分类器</strong>：使用带有权重的样本训练一个弱分类器。这个弱分类器可以是任何一种基本的机器学习模型，如决策树、线性分类器等，得到模型$G_m$（初始模型为$G_1$），通过这个带权重的训练过程，我们从初始模型$G_1$开始，经过$m-1$次迭代更新，最终得到了模型$G_m$。在每次迭代中，模型都会尝试更好地拟合那些具有较高权重（也就是在前一次迭代中预测错误）的样本。。</p></li><li><p><strong>计算</strong>$G_m$<strong>误分率</strong>：误分率是指弱分类器在所有样本上的错误分类的比例，它是由弱分类器的权重和指示函数（指示函数的值为1表示样本被错误分类，为0表示样本被正确分类）的乘积求和得到的。</p><p>$e_m&#x3D;\underset{i}{\overset{N}{\sum}} w_iI(y_i\neq{G_M(X_i))}$</p><p>其中：<br>$I(y_i\neq{G_M(X_i)})$</p><p>为指示函数，表示括号里成立时函数值为1，否则为0。</p></li><li><p><strong>计算弱分类器$G_m$的权重</strong>：弱分类器的权重（或称为系数）是基于其误分率计算的。误分率越低，弱分类器的权重就越大，这意味着它在最终的强分类器中的作用就越大。</p><p>$\alpha_m&#x3D;\frac{1}{2}\log{[\frac{1-e_m}{e_m}]}$</p></li><li><p><strong>更新样本权重</strong>：根据弱分类器的误分率$e$和当前的样本权重$w_m$，更新样本的权重。如果一个样本被正确分类，那么它的权重就会被降低；如果一个样本被错误分类，那么它的权重就会被提高。这样可以使得那些被错误分类的样本在后续的训练中得到更多的关注。</p><p>$w_{m+1,i}&#x3D;\frac{w_m}{z_m}\exp(-\alpha_my_iG_m(x_i))$</p><p>其中$z_m$为规范化因子（规范化因子$z_m$是用来确保权重$w_{m+1,i}$的总和为1的）：</p><p>$z_m&#x3D;\underset{i&#x3D;1}{\overset{m}{\sum}}w_{mi}\exp(-\alpha_my_iG_m(x_i))$</p><blockquote><p>规范化因子$z_m$是用来确保权重$w_{m+1,i}$的总和为1的。这是因为在许多机器学习算法中，如 AdaBoost，样本权重被解释为概率分布，因此它们的总和必须为1。</p></blockquote></li><li><p><strong>计算组合模型$f(x)&#x3D;\underset{m&#x3D;1}{\overset{M}{\sum}}\alpha_mG_m(x_i)$的误分率</strong>：组合模型是所有弱分类器的加权组合，其误分率是所有弱分类器的误分率的加权平均。每个弱分类器的权重就是其在组合模型中的作用。</p></li><li><p><strong>检查停止条件</strong>：如果组合模型的误分率低于一定的阈值，或者达到了最大的迭代次数，那么就停止迭代。否则，返回到步骤2，继续训练新的弱分类器。</p></li></ol><p>AdaBoost的目标是通过不断地迭代训练，找到一个误分率最低的强分类器。在每次迭代中，都会增加一个新的弱分类器，同时更新样本的权重，使得那些被错误分类的样本在后续的训练中得到更多的关注。这样，AdaBoost可以逐渐改进其分类性能，最终得到一个强大的分类器。</p><h2><span id="22-具体实现">2.2. 具体实现</span></h2><pre><code class="python">
# encoding=utf8
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier


# adaboost算法
class AdaBoost:
    &#39;&#39;&#39;
    input:n_estimators(int):迭代轮数
          learning_rate(float):弱分类器权重缩减系数
    &#39;&#39;&#39;

    def __init__(self, n_estimators=50, learning_rate=1.0):
        self.clf_num = n_estimators
        self.learning_rate = learning_rate

    def init_args(self, datasets, labels):
        self.X = datasets
        self.Y = labels
        self.M, self.N = datasets.shape
        # 弱分类器数目和集合
        self.clf_sets = []
        # 初始化weights
        self.weights = [1.0 / self.M] * self.M
        # G(x)系数 alpha
        self.alpha = []

    # ********* Begin *********#
    def _G(self, features, labels, weights):
        &#39;&#39;&#39;
        input:features(ndarray):数据特征
              labels(ndarray):数据标签
              weights(ndarray):样本权重系数
        &#39;&#39;&#39;
        e = 0
        for i in range(weights.shape[0]):
            if (labels[i] == self.G(self.X[i], self.clif_sets, self.alpha)):
                e += weights[i]
        return e

    # 计算alpha
    def _alpha(self, error):
        return 0.5 * np.log((1 - error) / error)

    # 规范化因子
    def _Z(self, weights, a, clf):
        return np.sum(weights * np.exp(-a * self.Y * self.G(self.X, clf, self.alpha)))

    # 权值更新
    def _w(self, a, clf, Z):
        w = np.zeros(self.weights.shape)
        for i in range(self.M):
            w[i] = weights[i] * np.exp(-a * self.Y[i] * G(x, clf, self.alpha)) / Z
        self.weights = w

    # G(x)的线性组合
    def G(self, x, v, direct):
        result = 0
        x = x.reshape(1, -1)
        for i in range(len(v)):
            result += v[i].predict(x) * direct[i]
        return result

    def fit(self, X, y):
        &#39;&#39;&#39;
        X(ndarray):训练数据
        y(ndarray):训练标签
        &#39;&#39;&#39;

        # 计算G(x)系数a
        self.init_args(X, y)
        &#39;&#39;&#39;
        for i in range(100):
            classifier = DecisionTreeClassifier(max_depth=3)
            classifier.fit(X, y)
            self.clf_sets.append(classifier)
            e = 0
            for i in range(len(self.weights)):
                temp = -1
                if classifier.predict(X[i].reshape(1,-1))&gt;0:
                    temp = 1
                if(self.Y[i] == temp):
                    e += self.weights[i]
            a = self._alpha(e)
            self.alpha.append(a)
            z = self._Z(self.weights, a, self.clf_sets)
            self._w(a, self.clf_sets, z)
        &#39;&#39;&#39;

        # 记录分类器

        # 规范化因子

        # 权值更新

    def predict(self, data):
        &#39;&#39;&#39;
        input:data(ndarray):单个样本
        output:预测为正样本返回+1，负样本返回-1
        &#39;&#39;&#39;
        ada = AdaBoostClassifier(n_estimators=100, learning_rate=0.1)
        ada.fit(self.X, self.Y)
        data = data.reshape(1, -1)
        predict = ada.predict(data)
        return predict[0]

    # ********* End *********#
</code></pre></div><div id="comment"><div id="twikoo-container"></div></div></div><footer id="footer"><div id="footer-wrap"><div>&copy; 2023 - 2024 Tianzhen <span id="footer-icon"><i class="fa-solid fa-font-awesome fa-fw"></i> </span>&commat;Tianzhen</div><div>Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp; <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a></div><div><a href="https://icp.gov.moe/?keyword=20236514" target="_blank">萌ICP备20236514号</a> <a target="_blank" rel="noopener" href="https://beian.miit.gov.cn">豫ICP备2023040339号</a></div></div><div><span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("11/07/2023 00:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="本站已运行 "+dnum+" 天 ",document.getElementById("times").innerHTML=hnum+" 小时 "+mnum+" 分 "+snum+" 秒"}setInterval("createtime()",250)</script></div></footer></div><transition name="fade"><div id="preview" ref="preview" v-show="previewShow"><img id="preview-content" ref="previewContent"></div></transition></div><script src="/api/js/main.js"></script><script>twikoo.init({el:"#twikoo-container",envId:"https://twikoo.lt514.com",region:"",path:location.pathname,lang:"zh"})</script><p style="text-align:center;margin:0" id="saintwei"></p><script>var saintwei=function(r){var i="気：",n=["现在开始天晴了","天空比大海还要深，是个未知的世界","天气因你逆转，世界因你天晴","能遇见你真是太好了","不管晴天还是雨天，我只是想和你相遇","我只是想再一次的见到她啊","是你让我找到了存在的意义","无论你在哪里，我一定拼尽全力去见你","拜托了，我们就这样一直在一起","只是天空的样子，就能改变人们的心情","天空上是另一个世界，自古如此","我从来不知道，渴望蓝天的人居然有那么多","天气真的是很不可思议，光只是天空的模样就让人感动不已","这是一个只有我和她知道，关于这世界的秘密","100%的晴天女孩","在充满暴风雨的世界，一起勇敢地爱下去","那年夏天，在那个天空之上的我们，把这个世界的样貌，彻底的改变了","天空比大海还要深，是个未知的世界","重要的人，想见的人，无论晴雨，不管多远，都一定要去见你","晴天里有阳光，阳光总是充满温馨，相信有这么多朋友的厚爱和鼓励，晴天会永远阳光灿烂","天晴得像一张蓝纸，几片薄薄的白云，像被阳光晒化了似的，随风缓缓浮游着","天空澄碧，纤云不染，远山含黛，和风送暖","浅蓝色的天幕，像一幅洁净的丝绒，镶着黄色的金边","一场飘洒的雨后，阳光带着清醒的空气飞来，试问是哪位仙子的生日，阳光如此美丽","晴天的午后，夏日的阳光如水般音符一样灿烂的流动，湿澈了不同的妩媚的忧伤","天那么蓝，连一丝浮絮都没有，像被过滤了一切杂色，瑰丽地熠熠发光","神明啊!求你!求你!求你!让我再见她一次","比起晴空，我更需要阳菜，天气什么的，就这样疯狂下去也可以"].map(function(t){return t}),l=["rgb(110,64,170)","rgb(150,61,179)","rgb(191,60,175)","rgb(228,65,157)","rgb(254,75,131)","rgb(255,94,99)","rgb(255,120,71)","rgb(251,150,51)","rgb(226,183,47)","rgb(198,214,60)","rgb(175,240,91)","rgb(127,246,88)","rgb(82,246,103)","rgb(48,239,130)","rgb(29,223,163)","rgb(26,199,194)","rgb(35,171,216)","rgb(54,140,225)","rgb(76,110,219)","rgb(96,84,200)"],a={text:"",prefixP:-5,skillI:0,skillP:0,direction:"forward",delay:2,step:1};!function t(){var e=n[a.skillI];a.step?a.step--:(a.step=1,a.prefixP<i.length?(0<=a.prefixP&&(a.text+=i[a.prefixP]),a.prefixP++):"forward"===a.direction?a.skillP<e.length?(a.text+=e[a.skillP],a.skillP++):a.delay?a.delay--:(a.direction="backward",a.delay=2):0<a.skillP?(a.text=a.text.slice(0,-1),a.skillP--):(a.skillI=(a.skillI+1)%n.length,a.direction="forward")),r.textContent=a.text,r.appendChild(function(t){for(var e=document.createDocumentFragment(),r=0;r<t;r++){var i=document.createElement("span");i.textContent=String.fromCharCode(94*Math.random()+33),i.style.color=l[Math.floor(Math.random()*l.length)],e.appendChild(i)}return e}(a.prefixP<i.length?Math.min(5,5+a.prefixP):Math.min(5,e.length-a.skillP))),setTimeout(t,75)}()};saintwei(document.getElementById("saintwei"))</script></body></html>